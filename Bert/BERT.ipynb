{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c9t6vp3OO8C"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfhYJ5qRaY53"
      },
      "outputs": [],
      "source": [
        "!pip install bert-tensorflow\n",
        "!pip install --upgrade bert\n",
        "!pip install tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiYwOJedOaOF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tqdm import *\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import AUC, Accuracy\n",
        "from sklearn.metrics import classification_report, multilabel_confusion_matrix, roc_auc_score, average_precision_score, recall_score, precision_score, matthews_corrcoef, accuracy_score, f1_score\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxU4AxTFYzXg"
      },
      "outputs": [],
      "source": [
        "url_data = (r\"https://raw.githubusercontent.com/conversationai/unhealthy-conversations/main/unhealthy_full.csv\")\n",
        "data_csv = pd.read_csv(url_data)\n",
        "data_csv_drop=data_csv.drop_duplicates(subset=\"comment\")\n",
        "data_wnull= data_csv_drop.dropna()\n",
        "print(data_wnull.shape)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "data_train, data_test= train_test_split(data_wnull, test_size=0.5, random_state=42)\n",
        "print(data_train.shape, data_test.shape)\n",
        "data_train.reset_index(drop=True, inplace=True)\n",
        "data_test.reset_index(drop=True, inplace=True)\n",
        "classes = [\"sarcastic\", \"healthy\", \"antagonize\", \"condescending\", \"dismissive\", \"hostile\"]\n",
        "y = data_train[classes].values\n",
        "\n",
        "train_sentences = data_train[\"comment\"].str.lower()\n",
        "test_sentences = data_test[\"comment\"].str.lower()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Qk8DtJcY5nk"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "padded_ids_train = []\n",
        "mask_ids_train = []\n",
        "\n",
        "for i in tqdm(range(len(train_sentences))):\n",
        "    encoding = tokenizer.encode_plus(train_sentences[i], max_length= 128, pad_to_max_length=True)\n",
        "    input_ids, attention_id = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "    padded_ids_train.append(input_ids)\n",
        "    mask_ids_train.append(attention_id)\n",
        "\n",
        "padded_ids_test = []\n",
        "mask_ids_test = []\n",
        "\n",
        "for i in tqdm(range(len(test_sentences))):\n",
        "    encoding=tokenizer.encode_plus(test_sentences[i], max_length=128, pad_to_max_length=True)\n",
        "    input_ids, attention_id = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "    padded_ids_test.append(input_ids)\n",
        "    mask_ids_test.append(attention_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODDecHPONS28"
      },
      "outputs": [],
      "source": [
        "train_id = np.array(padded_ids_train)\n",
        "train_mask = np.array(mask_ids_train)\n",
        "test_id = np.array(padded_ids_test)\n",
        "test_mask = np.array(mask_ids_test)\n",
        "\n",
        "input_1 = tf.keras.Input(shape = (128) , dtype=np.int32)\n",
        "input_2 = tf.keras.Input(shape = (128) , dtype=np.int32)\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-cased')\n",
        "output  = model([input_1 , input_2] , training = False )\n",
        "x = tf.keras.layers.Dense(128 , activation = tf.nn.relu )(output[0])  \n",
        "x = tf.keras.layers.Dropout(0.15)(x)                             \n",
        "x = tf.keras.layers.Dense(6 , activation = tf.nn.sigmoid )(x)\n",
        "model = tf.keras.Model(inputs = [input_1, input_2 ] , outputs = [x])\n",
        "model.summary()\n",
        "\n",
        "path= \"model_bert1.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath=path, monitor='val_precision', verbose=1, save_best_only=True, mode='max', save_weights_only=True)\n",
        "model.compile(optimizer=Adam(lr=3e-5),loss=tf.keras.losses.binary_crossentropy, metrics=tf.keras.metrics.Precision())\n",
        "\n",
        "# Training model...\n",
        "history = model.fit([train_id,train_mask], y, batch_size=32, epochs=5, callbacks=checkpoint, validation_split=0.1)\n",
        "\n",
        "# Loading model...\n",
        "model.load_weights('model_bert1.h5')\n",
        "y_pred = model.predict([test_id, test_mask])\n",
        "\n",
        "y_int = np.zeros_like(y_pred)\n",
        "y_int[y_pred > 0.5] = 1\n",
        "\n",
        "print('Classification report {}'.format(classification_report(y, y_int, zero_division=0)))\n",
        "print('Confusion matrix {}'.format(multilabel_confusion_matrix(y, y_int)))\n",
        "print('Accuracy is {}'.format(accuracy_score(y,y_int)))\n",
        "print('Roc-auc score is {}'.format(roc_auc_score(y, y_pred))) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCBQI5bfOnN-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFRRzf5u6hoK"
      },
      "source": [
        "# New Section"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}